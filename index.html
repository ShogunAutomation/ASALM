<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, user-scalable=yes" />
    <title>üè¢ Token Office Building - A Language Model Adventure</title>

    <!-- Basic SEO / social polish (safe defaults) -->
    <meta name="description" content="A playful, technically faithful metaphor for addressed state attention with shared slot keys: tokens come and go, roles persist." />
    <meta name="theme-color" content="#6b4423" />

    <link href="https://fonts.googleapis.com/css2?family=Courier+Prime:wght@400;700&family=Space+Mono:wght@400;700&family=DM+Sans:wght@400;500;700&display=swap" rel="stylesheet" />

    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }

        :root {
            --office-tan: #f4e8d8;
            --coffee-brown: #6b4423;
            --memo-blue: #4a90e2;
            --highlight-yellow: #ffd93d;
            --carpet-red: #c7493a;
            --sticky-note: #fff8dc;
            --ink-black: #2c2c2c;
            --paper-gray: #f5f5f5;
        }

        body {
            font-family: 'DM Sans', sans-serif;
            background: linear-gradient(135deg, var(--office-tan) 0%, #e8dcc8 100%);
            color: var(--ink-black);
            line-height: 1.6;
            overflow-x: hidden;
            -webkit-tap-highlight-color: transparent;
            touch-action: manipulation;
        }

        /* Token Queue Animation */
        .token-queue {
            position: fixed;
            top: 20px;
            right: -50px;
            z-index: 1000;
            display: flex;
            gap: 10px;
            animation: slideQueue 20s linear infinite;
            pointer-events: none;
            opacity: 0.98;
        }

        @keyframes slideQueue {
            0% { transform: translateX(0); }
            100% { transform: translateX(calc(-1 * (100vw + 2000px))); }
        }

        .token-in-queue {
            background: var(--sticky-note);
            border: 2px solid var(--coffee-brown);
            padding: 8px 12px;
            border-radius: 4px;
            font-family: 'Courier Prime', monospace;
            font-size: 14px;
            white-space: nowrap;
            box-shadow: 2px 2px 4px rgba(0,0,0,0.1);
        }

        /* Header */
        header {
            background: var(--coffee-brown);
            color: var(--office-tan);
            padding: 60px 20px 40px;
            text-align: center;
            position: relative;
            overflow: hidden;
        }

        header::before {
            content: "üè¢üè¢üè¢üè¢üè¢üè¢üè¢üè¢üè¢üè¢";
            position: absolute;
            top: 10px;
            left: 0;
            right: 0;
            font-size: 30px;
            opacity: 0.2;
            animation: floatBuildings 30s linear infinite;
            pointer-events: none;
        }

        @keyframes floatBuildings {
            0% { transform: translateX(0); }
            100% { transform: translateX(-50%); }
        }

        h1 {
            font-family: 'Space Mono', monospace;
            font-size: 3em;
            margin-bottom: 20px;
            text-shadow: 3px 3px 0 rgba(0,0,0,0.3);
        }

        .subtitle {
            font-size: 1.3em;
            margin-bottom: 30px;
            opacity: 0.9;
        }

        /* Interactive Token Button */
        .token-generator {
            position: fixed;
            bottom: 30px;
            right: 30px;
            z-index: 1000;
        }

        .token-btn {
            background: var(--highlight-yellow);
            border: 3px solid var(--ink-black);
            padding: 15px 25px;
            font-family: 'Space Mono', monospace;
            font-size: 16px;
            font-weight: bold;
            cursor: pointer;
            border-radius: 8px;
            box-shadow: 5px 5px 0 var(--ink-black);
            transition: all 0.2s;
            -webkit-tap-highlight-color: transparent;
            touch-action: manipulation;
            min-height: 48px;
        }

        .token-btn:hover {
            transform: translate(2px, 2px);
            box-shadow: 3px 3px 0 var(--ink-black);
        }

        .token-btn:active {
            transform: translate(5px, 5px);
            box-shadow: 0 0 0 var(--ink-black);
        }

        /* Elevator Animation */
        .elevator-shaft {
            position: fixed;
            left: 20px;
            top: 100px;
            width: 60px;
            height: 500px;
            background: linear-gradient(to bottom, #8b7355 0%, #5d4a3a 100%);
            border: 3px solid var(--ink-black);
            border-radius: 8px;
            overflow: hidden;
        }

        .elevator {
            width: 50px;
            height: 60px;
            background: var(--memo-blue);
            border: 2px solid var(--ink-black);
            border-radius: 4px;
            position: absolute;
            left: 5px;
            animation: elevatorMove 8s ease-in-out infinite;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 20px;
        }

        @keyframes elevatorMove {
            0%, 10% { top: 430px; }
            45%, 55% { top: 10px; }
            90%, 100% { top: 430px; }
        }

        /* Main Content */
        .container {
            max-width: 900px;
            margin: 40px auto;
            padding: 0 20px 100px;
        }

        .content-section {
            background: white;
            border: 3px solid var(--ink-black);
            border-radius: 8px;
            padding: 40px;
            margin-bottom: 30px;
            box-shadow: 8px 8px 0 rgba(0,0,0,0.1);
            position: relative;
        }

        .content-section::before {
            content: "üìÑ";
            position: absolute;
            top: -15px;
            right: 20px;
            font-size: 30px;
            transform: rotate(15deg);
        }

        h2 {
            font-family: 'Space Mono', monospace;
            font-size: 2em;
            margin-bottom: 20px;
            color: var(--coffee-brown);
            border-bottom: 3px solid var(--highlight-yellow);
            padding-bottom: 10px;
        }

        h3 {
            font-family: 'Space Mono', monospace;
            font-size: 1.4em;
            margin: 30px 0 15px;
            color: var(--memo-blue);
        }

        p { margin-bottom: 20px; }

        code {
            background: #f5f5f5;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier Prime', monospace;
            font-size: 0.9em;
        }
        
        pre {
            background: #2c2c2c;
            color: #f8f8f8;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 20px 0;
            font-family: 'Courier Prime', monospace;
            border: 2px solid var(--ink-black);
        }

pre code {
    color: #f8f8f8;
    background: transparent;
    padding: 0;
}

pre code::selection,
pre::selection {
    background: #555;
    color: #ffffff;
}


        /* Comment Section Styling */
        .comment {
            background: var(--sticky-note);
            border-left: 4px solid var(--carpet-red);
            padding: 20px;
            margin: 20px 0;
            border-radius: 4px;
            box-shadow: 3px 3px 0 rgba(0,0,0,0.1);
            transition: transform 0.2s;
            word-wrap: break-word;
            overflow-wrap: break-word;
        }

        .comment:hover { transform: translateX(5px); }
        @media (hover: none) { .comment:hover { transform: none; } }

        .comment-header {
            font-weight: bold;
            margin-bottom: 10px;
            display: flex;
            justify-content: space-between;
            align-items: center;
            gap: 10px;
        }

        .token-name {
            font-family: 'Space Mono', monospace;
            color: var(--coffee-brown);
        }

        .star-rating { font-size: 1.2em; }

        .official-response {
            background: var(--memo-blue);
            color: white;
            border-left: 4px solid var(--ink-black);
        }

        /* Interactive Building Visualization */
        .building-viz {
            display: grid;
            grid-template-columns: repeat(1, 1fr);
            gap: 5px;
            margin: 30px 0;
            padding: 20px;
            background: linear-gradient(to bottom, #87ceeb 0%, #f4e8d8 100%);
            border: 3px solid var(--ink-black);
            border-radius: 8px;
        }

        .floor {
            background: var(--office-tan);
            border: 2px solid var(--ink-black);
            padding: 15px;
            border-radius: 4px;
            cursor: pointer;
            transition: all 0.25s;
            position: relative;
            min-height: 48px;
            touch-action: manipulation;
            -webkit-tap-highlight-color: transparent;
        }

        .floor:hover, .floor:active {
            background: var(--highlight-yellow);
            transform: scale(1.03);
        }
        @media (hover: none) { .floor:active { background: var(--highlight-yellow); } }

        .floor-number {
            font-family: 'Space Mono', monospace;
            font-weight: bold;
            font-size: 1.2em;
        }

        .floor-info {
            font-size: 0.9em;
            margin-top: 5px;
            opacity: 0.85;
        }

        /* Slot Notebooks */
        .notebook-display {
            display: flex;
            gap: 10px;
            flex-wrap: wrap;
            margin: 20px 0;
        }

        .notebook {
            width: 80px;
            height: 100px;
            background: linear-gradient(135deg, #ff6b6b 0%, #ff8e53 100%);
            border: 2px solid var(--ink-black);
            border-radius: 4px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 30px;
            cursor: pointer;
            transition: transform 0.25s;
            box-shadow: 3px 3px 0 rgba(0,0,0,0.2);
            touch-action: manipulation;
            -webkit-tap-highlight-color: transparent;
            user-select: none;
        }

        .notebook:hover, .notebook:active { transform: translateY(-8px) rotate(4deg); }
        @media (hover: none) { .notebook:active { transform: translateY(-5px) rotate(3deg); } }

        .notebook.shared { background: linear-gradient(135deg, #4a90e2 0%, #63b3ed 100%); }
        .notebook.global { background: linear-gradient(135deg, #48bb78 0%, #68d391 100%); }

        /* Floating tokens */
        .floating-token {
            position: fixed;
            font-family: 'Courier Prime', monospace;
            font-size: 20px;
            pointer-events: none;
            animation: floatUp 4s ease-out forwards;
            z-index: 999;
            text-shadow: 1px 1px 0 rgba(0,0,0,0.15);
        }

        @keyframes floatUp {
            0% { opacity: 1; transform: translateY(0) rotate(0deg); }
            100% { opacity: 0; transform: translateY(-300px) rotate(360deg); }
        }

        /* Token Satisfaction Meter */
        .satisfaction-meter {
            background: white;
            border: 3px solid var(--ink-black);
            border-radius: 8px;
            padding: 20px;
            margin: 30px 0;
        }

        .meter-bar {
            height: 30px;
            background: var(--office-tan);
            border: 2px solid var(--ink-black);
            border-radius: 15px;
            overflow: hidden;
            margin: 10px 0;
        }

        .meter-fill {
            height: 100%;
            background: linear-gradient(90deg, var(--carpet-red) 0%, var(--highlight-yellow) 50%, #48bb78 100%);
            transition: width 1s ease-out;
            display: flex;
            align-items: center;
            justify-content: flex-end;
            padding-right: 10px;
            color: white;
            font-weight: bold;
            white-space: nowrap;
        }

        /* Typewriter effect */
        .typewriter {
            overflow: hidden;
            border-right: 3px solid var(--ink-black);
            white-space: nowrap;
            animation: typing 3s steps(40, end), blink-caret 0.75s step-end infinite;
            display: inline-block;
            max-width: 100%;
        }

        @keyframes typing { from { width: 0; } to { width: 100%; } }
        @keyframes blink-caret { from, to { border-color: transparent; } 50% { border-color: var(--ink-black); } }

        /* Sticky Note */
        .sticky-note {
            background: var(--sticky-note);
            border: 1px solid #e5d5a0;
            padding: 20px;
            margin: 20px 0;
            box-shadow: 3px 3px 8px rgba(0,0,0,0.2);
            transform: rotate(-1deg);
            font-family: 'Courier Prime', monospace;
        }

        ul, ol { margin: 20px 0; padding-left: 40px; }
        li { margin: 10px 0; }

        strong { color: var(--coffee-brown); }
        em { color: var(--memo-blue); }

        hr {
            border: none;
            border-top: 3px dashed var(--ink-black);
            margin: 40px 0;
        }

        .info-box {
            background: #e8f4f8;
            border-left: 5px solid var(--memo-blue);
            padding: 20px;
            margin: 20px 0;
            border-radius: 4px;
        }

        .legend-box {
            background: var(--paper-gray);
            border: 2px solid var(--ink-black);
            border-radius: 8px;
            padding: 15px;
            margin: 20px 0;
            font-size: 0.9em;
        }

        /* Inline tooltip (replaces modal) */
        .info-tooltip {
            position: fixed;
            background: white;
            border: 3px solid var(--ink-black);
            border-radius: 8px;
            box-shadow: 8px 8px 0 rgba(0,0,0,0.2);
            padding: 16px;
            max-width: 400px;
            z-index: 2000;
            opacity: 0;
            transform: scale(0.95) translateY(-10px);
            transition: opacity 0.25s ease, transform 0.25s ease;
            pointer-events: none;
            font-size: 0.95em;
        }

        .info-tooltip.show {
            opacity: 1;
            transform: scale(1) translateY(0);
        }

        .info-tooltip-header {
            font-family: 'Space Mono', monospace;
            font-weight: bold;
            font-size: 1.1em;
            margin-bottom: 10px;
            color: var(--coffee-brown);
            border-bottom: 2px solid var(--highlight-yellow);
            padding-bottom: 6px;
        }

        .info-tooltip-body p {
            margin-bottom: 10px;
            line-height: 1.5;
        }

        .info-tooltip-body ul {
            margin: 10px 0 10px 20px;
        }

        .info-tooltip-body li {
            margin: 5px 0;
        }

        .info-tooltip-body code {
            background: var(--paper-gray);
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier Prime', monospace;
            font-size: 0.9em;
        }

        .info-tooltip-body em {
            color: var(--memo-blue);
        }

        /* Make interactive elements show they're clickable */
        .floor, .notebook {
            position: relative;
        }

        .floor::after {
            content: 'üëÜ';
            position: absolute;
            top: 8px;
            right: 8px;
            font-size: 16px;
            opacity: 0;
            transition: opacity 0.2s;
        }

        .notebook::after {
            content: 'üëÜ';
            position: absolute;
            top: 5px;
            right: 5px;
            font-size: 14px;
            opacity: 0;
            transition: opacity 0.2s;
        }

        .floor:hover::after, .notebook:hover::after {
            opacity: 0.6;
        }

        @media (max-width: 768px) {
            .info-tooltip {
                max-width: calc(100vw - 40px);
                font-size: 0.9em;
                left: 20px !important;
                right: 20px !important;
                width: auto !important;
            }

            .floor::after, .notebook::after {
                opacity: 0.4;
            }
        }

        /* Mobile */
        @media (max-width: 768px) {
            h1 { font-size: 1.8em; line-height: 1.2; }
            .subtitle { font-size: 0.95em; line-height: 1.4; }
            .content-section { padding: 20px 15px; margin-bottom: 20px; }
            .elevator-shaft { display: none; }

            .token-queue { top: 10px; }
            .token-in-queue { padding: 6px 10px; font-size: 12px; }

            .token-generator {
                bottom: 15px;
                right: 15px;
                left: 15px;
                text-align: center;
            }

            .token-btn {
                padding: 15px 25px;
                font-size: 16px;
                width: 100%;
                max-width: 320px;
            }

            h2 { font-size: 1.5em; line-height: 1.3; }
            h3 { font-size: 1.2em; line-height: 1.3; }

            .typewriter { font-size: 0.9em; white-space: normal; border-right: none; animation: none; }

            .building-viz { padding: 15px 10px; }
            .floor { padding: 12px; }
            .floor-number { font-size: 1em; }
            .floor-info { font-size: 0.85em; }

            .notebook-display { justify-content: center; }
            .notebook { width: 70px; height: 90px; font-size: 25px; }

            .comment { padding: 15px; margin: 15px 0; }
            .comment-header { flex-direction: column; gap: 5px; align-items: flex-start; }

            pre { padding: 15px; font-size: 0.85em; }
            .sticky-note { padding: 15px; font-size: 0.9em; }
            .satisfaction-meter { padding: 15px; }
            .meter-bar { height: 25px; }
            .meter-fill { font-size: 0.9em; padding-right: 8px; }

            ul, ol { padding-left: 25px; }
            li { margin: 8px 0; }
            .info-box { padding: 15px; font-size: 0.95em; }
        }

        @media (max-width: 480px) {
            h1 { font-size: 1.5em; }
            header { padding: 40px 15px 30px; }
            .subtitle { font-size: 0.85em; }
            .container { padding: 0 10px 120px; }
            .content-section { padding: 15px 12px; }
            h2 { font-size: 1.3em; }
            h3 { font-size: 1.1em; }
            .notebook { width: 60px; height: 80px; font-size: 20px; }
            .token-btn { padding: 12px 20px; font-size: 15px; }
        }

        @media (prefers-reduced-motion: reduce) {
            * { animation: none !important; transition: none !important; scroll-behavior: auto !important; }
            .token-queue, .elevator { display: none !important; }
            .typewriter { border-right: none; white-space: normal; }
        }
    </style>
</head>

<body>
    <!-- Token Queue Animation -->
    <div class="token-queue" aria-hidden="true">
        <div class="token-in-queue">"the"</div>
        <div class="token-in-queue">"Stanford"</div>
        <div class="token-in-queue">"is"</div>
        <div class="token-in-queue">"a"</div>
        <div class="token-in-queue">"university"</div>
        <div class="token-in-queue">";"</div>
        <div class="token-in-queue">"it"</div>
        <div class="token-in-queue">"has"</div>
        <div class="token-in-queue">"many"</div>
        <div class="token-in-queue">"students"</div>
    </div>

    <!-- Elevator -->
    <div class="elevator-shaft" aria-hidden="true">
        <div class="elevator">üõó</div>
    </div>

    <header>
        <h1>üè¢ The Token Office Building</h1>
        <p class="subtitle">How Addressed State Attention Actually Works<br />(And Why Roles Matter More Than Tokens)</p>
        <div class="typewriter">One token at a time, from ground floor to the top...</div>
    </header>

    <div class="container">
        <div class="content-section">
            <h2>Welcome to the Building! üëã</h2>
            <p>Most language models are explained the same way: tokens go in, attention mixes them, layers stack, logits come out. That explanation works‚Äîbut it hides something important about how language modeling actually happens.</p>

            <p>Over the last several weeks, I've been working with a language model built around <strong>addressed state attention with shared slot keys</strong>. It's small (~50M parameters), trained on raw WikiText, and yet it displays behavior that doesn't fit neatly into the usual "Transformer intuition." This post is an attempt to explain why‚Äîusing a concrete mental model that matches what we actually observe in analysis.</p>

            <p><strong>The story begins with an office building.</strong></p>
        </div>

        <div class="content-section">
            <h2>üèóÔ∏è The Building: A 10-Story Language Processing Facility</h2>

            <p>Imagine a 10-story office building designed specifically to process language. Outside, there's a long line of tokens‚Äîwords, punctuation, fragments‚Äîpatiently waiting to be processed. Only <strong>one token enters at a time</strong>, and each token must travel from the ground floor all the way to the top before the model decides what comes next.</p>

            <div class="sticky-note">
                <strong>‚öôÔ∏è Implementation Note:</strong><br />
                In practice, we run the full prefix in parallel on GPU. But the slot states are updated with a causal prefix-scan: the state at position <code>t</code> is a running, normalized summary of tokens <code>0..t</code>.
            </div>

            <p>Each token arrives with a blank sticky note attached to it. That sticky note will be updated as the token climbs upward, accumulating context and decisions from every floor it visits.</p>

            <h3>üìö What's on Each Floor</h3>

            <p>Every floor in this building has:</p>
            <ul>
                <li><strong>One room</strong> (the layer)</li>
                <li><strong>Four workers inside</strong> (attention heads)</li>
                <li><strong>Eight notebooks per worker</strong>, divided like this:
                    <ul>
                        <li>4 private notebooks (only that worker uses them)</li>
                        <li>2 floor-shared notebooks (shared <em>tab labels</em> across all four workers on the same floor)</li>
                        <li>2 building-shared notebooks (shared <em>tab labels</em> across all workers on all floors)</li>
                    </ul>
                </li>
            </ul>

            <div class="notebook-display" role="group" aria-label="Notebook types">
                <div class="notebook" data-kind="private" title="Private notebook (per head)" aria-label="Private notebook">üìï</div>
                <div class="notebook" data-kind="private" title="Private notebook (per head)" aria-label="Private notebook">üìï</div>
                <div class="notebook" data-kind="private" title="Private notebook (per head)" aria-label="Private notebook">üìï</div>
                <div class="notebook" data-kind="private" title="Private notebook (per head)" aria-label="Private notebook">üìï</div>
                <div class="notebook shared" data-kind="layer" title="Layer-shared keyset (per layer)" aria-label="Layer-shared notebook">üìò</div>
                <div class="notebook shared" data-kind="layer" title="Layer-shared keyset (per layer)" aria-label="Layer-shared notebook">üìò</div>
                <div class="notebook global" data-kind="global" title="Global keyset (across layers)" aria-label="Global notebook">üìó</div>
                <div class="notebook global" data-kind="global" title="Global keyset (across layers)" aria-label="Global notebook">üìó</div>
            </div>

            <div class="legend-box">
                <strong>üìö Notebook Legend:</strong><br />
                üìï Private (per head) ‚Ä¢ üìò Shared tab labels (per layer) ‚Ä¢ üìó Shared tab labels (global)
            </div>

            <p>Here‚Äôs the crucial detail about how sharing works: workers on higher floors use <strong>shared tab labels</strong> (slot keys)‚Äîsome shared across heads on a floor, and a small set shared across the whole building‚Äîto keep role meanings aligned. They don‚Äôt literally inherit pages from lower floors. What rises up the building is the token‚Äôs <strong>sticky note</strong> (the residual stream). The shared tab system is what keeps ‚Äúslot #7‚Äù meaningfully comparable across heads and layers.</p>

            <div class="building-viz" id="buildingViz" aria-label="Building floor map">
                <!-- populated by JS -->
            </div>
        </div>

        <div class="content-section">
            <h2>üö∂ Step 1: The Token Enters the Room</h2>

            <p>When a token walks into a floor:</p>
            <ul>
                <li>All four workers look at it simultaneously</li>
                <li>They also examine:
                    <ul>
                        <li>The token‚Äôs <strong>sticky note</strong> (decisions made by earlier floors)</li>
                        <li>The <strong>shared tab labels</strong> (slot keys) that keep role meanings aligned</li>
                    </ul>
                </li>
            </ul>

            <p>Each worker has a different specialty, and those specialties change by floor. On lower floors, workers care about <em>what</em> the token means. On middle floors, they care about <em>how</em> it fits grammatically. On upper floors, they care about <em>what it means here, now</em>.</p>

            <h3>üíª In Code Terms</h3>

            <pre><code># Token arrives with an embedding
x = token_embedding  # [batch, time, embed_dim]

# Plus context from previous layers (the "sticky note")
# This is the residual stream</code></pre>
        </div>
        
        <div class="content-section">
            <h2>‚úçÔ∏è Step 2: Writing to Notebooks (Soft, Not Discrete)</h2>

            <p>Each worker writes <strong>softly</strong> into several notebooks at once. This is not a binary decision.</p>

            <div class="info-box">
                <strong>Important nuance:</strong>
                <ul>
                    <li>Workers do <strong>not</strong> choose exactly one notebook</li>
                    <li>They distribute weight across many</li>
                    <li>Some notebooks receive more weight, others less</li>
                    <li>The distribution is learned, not hardcoded</li>
                </ul>
            </div>

            <p>In practice, a worker might:</p>
            <ul>
                <li>Lightly update many slots</li>
                <li>Strongly update one or two</li>
                <li>Ignore others entirely</li>
            </ul>

            <p>This explains several observed properties:</p>
            <ul>
                <li>Routing entropy stays <strong>high</strong> (never collapses to one-hot)</li>
                <li>Slot usage is <strong>balanced but non-uniform</strong></li>
                <li>No hard specialization dead-ends emerge</li>
            </ul>

            <p><strong>This is not mixture-of-experts gating.</strong> It‚Äôs closer to continuous role blending.</p>

            <h3>üîç What Actually Persists</h3>

            <p>Here‚Äôs the key question: when a token leaves the building, what stays behind?</p>

            <h4>Two Kinds of Memory</h4>

            <p><strong>1. Token-local state (ephemeral):</strong></p>
            <ul>
                <li>The token embedding</li>
                <li>The sticky note (routing/refinement context)</li>
                <li>Per-token read weights</li>
            </ul>

            <p>This information:</p>
            <ul>
                <li>Exists only while the token is inside the building</li>
                <li>Is never revisited after the token leaves</li>
                <li>Does not persist across tokens</li>
            </ul>

            <p><strong>2. Slot state (persistent within a prefix):</strong></p>
            <ul>
                <li>Slot states are continuously updated as we move from token position to token position</li>
                <li>Within a single forward pass, they persist and accumulate a normalized running summary of everything seen so far</li>
                <li>Between independent sequences they reset‚Äîunless we cache and continue prefix state during generation</li>
            </ul>

            <div class="sticky-note">
                <strong>üí° Key Insight:</strong> A slot is not a neuron, and not a token. A slot is a <strong>role accumulator</strong>.
            </div>

            <pre><code># Computing write weights (which notebooks get updated)
write_logits  = einsum(slot_keys, token_keys)
write_weights = softmax(write_logits / temperature)

# Updating slot state (writing into notebooks)
# This is a streaming, causal operation
slot_state = weighted_prefix_sum(write_weights, token_values)</code></pre>

            <p>Crucially:</p>
            <ul>
                <li>Slot states are updated <strong>causally</strong></li>
                <li>They are <strong>never reset</strong> between tokens within a sequence</li>
                <li>They evolve <strong>smoothly</strong> over time</li>
            </ul>

            <p>When a token leaves the building, the token is gone. But the <strong>notebooks remain</strong>, now containing a little more information than before.</p>
        </div>
        
        

        <div class="content-section" id="step3">
            <h2>üìñ Step 3: Reading from Notebooks (Role-Based, Not Token-Based)</h2>

            <p>When a worker reads from notebooks, they‚Äôre not looking at other tokens. They‚Äôre looking at <strong>roles encoded in slot state</strong>.</p>

            <pre><code># Reading from slots (which notebooks to consult)
read_logits  = dot(query, slot_keys)
read_weights = softmax(read_logits / temperature)
output       = sum(read_weights * slot_state)</code></pre>

            <p>Notice what changes versus classic token attention:</p>
            <ul>
                <li>The token never ‚Äúlooks at‚Äù other tokens directly</li>
                <li>It looks at <strong>roles</strong> encoded in slots</li>
                <li>Slots contain a distilled, streaming summary of history</li>
            </ul>

            <p>This is why the model doesn‚Äôt need quadratic attention over the full sequence. Instead of <code>O(T¬≤)</code> token-to-token attention, we have <code>O(T √ó K)</code> token-to-slot access, where <code>K</code> (slots) is typically 32‚Äì64 and stays constant as context grows.</p>
        </div>

        <div class="content-section">
            <h2>üè¢ Step 4: Floors Have Different Jobs (Emergent Specialization)</h2>

            <p>This is where the model‚Äôs behavior becomes especially interesting. The workers on different floors develop different specialties‚Äînot because we told them to, but because it‚Äôs an efficient decomposition of next-token prediction.</p>

            <h3>üå± Lower Floors: Content First</h3>

            <p><strong>On early floors (e.g., layer 0):</strong></p>
            <ul>
                <li>Workers mostly care about <strong>content</strong></li>
                <li>Named entities, topics, surface meaning dominate</li>
                <li>Structure is weak or incidental</li>
            </ul>

            <p>These layers answer: <strong>"What is this token about?"</strong></p>

            <h3>üèóÔ∏è Middle Floors: Structure Takes Over</h3>

            <p><strong>On middle floors (e.g., layers 3‚Äì5):</strong></p>
            <ul>
                <li>Workers shift attention to <strong>syntax and structure</strong></li>
                <li>Function words, punctuation, grammar dominate</li>
                <li>Layer-shared and global-shared keysets become especially useful for stable roles</li>
            </ul>

            <p>These layers answer: <strong>"How does this token fit grammatically?"</strong></p>

            <h3>üéØ Upper Floors: Integration and Meaning-in-Context</h3>

            <p><strong>On higher floors (e.g., layers 6‚Äì9):</strong></p>
            <ul>
                <li>Content returns‚Äîbut <strong>transformed</strong></li>
                <li>Tokens are interpreted in context, not isolation</li>
                <li>Slots mix structural and semantic roles again</li>
            </ul>

            <p>These layers answer: <strong>"What does this token mean here, now?"</strong></p>
        </div>

        <div class="content-section">
            <h2>üìù Step 5: Updating the Sticky Note (Routing)</h2>

            <p>After all workers finish writing and reading, something interesting happens.</p>

            <p>A <strong>coordinator</strong> computes‚Äîon this floor‚Äîhow strongly this token should consult each role notebook. The coordinator looks at:</p>
            <ul>
                <li><strong>Not</strong> the notes themselves</li>
                <li>Just <strong>which roles mattered</strong>, and <strong>how much</strong></li>
            </ul>

            <p>This reading pattern contributes to the token‚Äôs updated sticky note (its new residual-stream representation):</p>
            <ul>
                <li>Emphasizing certain roles</li>
                <li>De-emphasizing others</li>
                <li>Shaping the representation that moves to the next floor</li>
            </ul>

            <div class="info-box">
                <strong>This is what routing really is in this model:</strong><br />
                Not selecting content ‚Äî selecting <strong>roles</strong>
            </div>

            <p>The routing weights don‚Äôt tell us <em>what</em> the token saw, but <em>how</em> it used available information channels. Importantly, routing is recomputed at each floor based on the token‚Äôs current representation‚Äîit's not a static instruction carried upward.</p>
        </div>

        <div class="content-section">
            <h2>üõó Step 6: Refinement Happens in the Elevator</h2>

            <p>As the token rides the elevator between floors, there‚Äôs a quiet observer watching everything.</p>

            <p>This observer:</p>
            <ul>
                <li>Watches routing patterns over time</li>
                <li>Never sees the token itself</li>
                <li>Only sees <strong>how roles are being used</strong></li>
            </ul>

            <p>This is the key insight: refinement is <strong>meta</strong> because it adjusts based on how you routed (which notebooks you consulted), not raw token content. Crucially, <strong>refinement sees read_weights, not slot contents</strong>.</p>

            <p>Occasionally, the observer nudges the sticky note:</p>
            <ul>
                <li>Suppress redundancy</li>
                <li>Amplify underused structure</li>
                <li>Stabilize drifting behavior</li>
            </ul>

            <p>This corresponds to <strong>slot-space refinement</strong>:</p>

            <pre><code># Refinement operates on read weights (not content)
u     = slot_in(read_weights)           # compress read pattern
delta = slot_out(linear_attention(u))   # refine in lower-dim space
output += gate * delta                  # add back gated correction</code></pre>

            <p><strong>Interpretation:</strong></p>
            <ul>
                <li>The model reflects on <strong>how it used roles</strong></li>
                <li>Not on <strong>what the roles contained</strong></li>
                <li>Adjustments are typically stabilizing (and should remain bounded)</li>
            </ul>

            <div class="sticky-note">
                <strong>Importantly:</strong><br />
                ‚Ä¢ Turning refinement off <strong>can</strong> degrade generation<br />
                ‚Ä¢ Refinement should not <strong>overwhelm</strong> base behavior<br />
                ‚Ä¢ It‚Äôs corrective, not controlling
            </div>
        </div>

        <div class="content-section">
            <h2>üèÜ The Top Floor: Choosing the Next Token</h2>

            <p>At decision time, the ‚Äúboss‚Äù (the output head) doesn‚Äôt see raw past tokens directly. The only things available are:</p>
            <ol>
                <li>The current token‚Äôs representation after the stack (the accumulated sticky note)</li>
                <li>Whatever structure the model has distilled into internal role channels while scanning the prefix</li>
            </ol>

            <p>The output head maps that final representation to a distribution over the vocabulary.</p>

            <p>When the token finally reaches the top floor:</p>
            <ul>
                <li>The sticky note encodes <strong>layered context</strong> from all floors</li>
                <li>Slot state holds <strong>prefix summaries</strong> organized by roles</li>
                <li>Private capacity can capture <strong>local nuance</strong> without breaking shared conventions</li>
            </ul>

            <p>The model then answers a single question:</p>

            <blockquote style="background: var(--office-tan); border-left: 4px solid var(--coffee-brown); padding: 20px; margin: 20px 0; font-style: italic;">
                If the line were one token longer, which of the 50,000+ tokens would fit best here?
            </blockquote>

            <pre><code>x        = layer_norm(x)      # final normalization
logits   = lm_head(x)         # [batch, time, vocab_size]
next_tok = sample(logits[:, -1, :])</code></pre>

            <p>Then the next token enters the building, and the whole process repeats.</p>
        </div>

        <div class="content-section">
            <h2>üîë Why Shared Slots Matter (A Lot)</h2>

            <p>Let‚Äôs return to those notebooks. Remember how some notebooks are private, some share tab labels within a floor, and some share tab labels across the entire building?</p>

            <p>Analysis often shows a layerwise shift:</p>

            <table style="width: 100%; border-collapse: collapse; margin: 20px 0;">
                <thead>
                    <tr style="background: var(--coffee-brown); color: white;">
                        <th style="padding: 12px; border: 2px solid var(--ink-black); text-align: left;">Layer Range</th>
                        <th style="padding: 12px; border: 2px solid var(--ink-black); text-align: left;">Typical Emphasis</th>
                    </tr>
                </thead>
                <tbody>
                    <tr style="background: white;">
                        <td style="padding: 12px; border: 2px solid var(--ink-black);">Lower layers</td>
                        <td style="padding: 12px; border: 2px solid var(--ink-black);">Content-heavy (entities, topics)</td>
                    </tr>
                    <tr style="background: var(--office-tan);">
                        <td style="padding: 12px; border: 2px solid var(--ink-black);">Mid layers</td>
                        <td style="padding: 12px; border: 2px solid var(--ink-black);">Structure-heavy (syntax, function words)</td>
                    </tr>
                    <tr style="background: white;">
                        <td style="padding: 12px; border: 2px solid var(--ink-black);">Upper layers</td>
                        <td style="padding: 12px; border: 2px solid var(--ink-black);">Mixed integration (meaning-in-context)</td>
                    </tr>
                </tbody>
            </table>

            <p>This emerges because of <strong>shared addressing</strong>.</p>

            <h3>Layer-Shared Keysets</h3>
            <ul>
                <li>Encourage consistency across heads</li>
                <li>Stabilize grammatical roles</li>
                <li>Reduce ‚Äúhead drift‚Äù by giving everyone a common role vocabulary</li>
            </ul>

            <h3>Global Keysets</h3>
            <ul>
                <li>Provide a small, persistent role vocabulary across the full stack</li>
                <li>Can support long-range discourse regularities (topic continuity, mode shifts, etc.)</li>
            </ul>

            <p>In code, this is visible in how slot keys are constructed:</p>

            <pre><code>slot_keys = concat(
    private_keys,      # [H, K_private, d]
    layer_shared_keys, # [K_layer, d]  (broadcast across heads)
    global_shared_keys # [K_global, d] (broadcast across heads and layers)
)</code></pre>

            <p>This single concatenation explains a lot of behavior. Workers have access to private scratchpads and shared role conventions, and they learn when to use which.</p>
        </div>

        <div class="content-section">
            <h2>üåü Why Structure Emerges Without Supervision</h2>

            <p>No slot is labeled ‚Äúnoun‚Äù or ‚Äúverb‚Äù or ‚Äúsubject.‚Äù</p>

            <p>Structure emerges because:</p>
            <ul>
                <li><strong>Structure is reusable</strong> ‚Äî the same patterns apply everywhere</li>
                <li><strong>Structure compresses history efficiently</strong> ‚Äî roles summarize more than raw token lists</li>
                <li><strong>Structure improves predictability</strong> ‚Äî knowing ‚Äúwe‚Äôre in a prepositional phrase‚Äù constrains what comes next</li>
            </ul>

            <p>Slots that encode structure:</p>
            <ul>
                <li>Get <strong>reused</strong> frequently</li>
                <li><strong>Accumulate mass</strong> (higher write/read weight)</li>
                <li>Become <strong>stable attractors</strong> in slot space</li>
            </ul>

            <p>This is selection pressure from the objective, not hard coding.</p>
        </div>

        <div class="content-section">
            <h2>ü§î Why This Is a Different Kind of Model</h2>

            <p>At no point does this system:</p>
            <ul>
                <li>Store a single ‚Äúmeaning‚Äù for a token</li>
                <li>Commit to one role</li>
                <li>Require a quadratic attention map between tokens</li>
            </ul>

            <p>Instead:</p>
            <ul>
                <li><strong>Meaning is distributed</strong> across multiple slots</li>
                <li><strong>Roles are soft and reusable</strong> across time and context</li>
                <li><strong>Structure and content trade dominance naturally by layer</strong></li>
            </ul>

            <p>This helps explain why you can see:</p>
            <ul>
                <li>Surprisingly strong performance at modest scale (efficient capacity use)</li>
                <li>Healthy entropy (no dead slots, no brittle gating)</li>
                <li>Emergent structure without explicit labels</li>
            </ul>
        </div>

        <!-- (Your remaining sections unchanged for brevity of this ‚Äúfinal pass‚Äù update)
             Keeping your Transformer vs not-Transformer, MoE discussion, and closing reflection as-is,
             with minor textual tightening already incorporated above. -->

        <div class="content-section">
            <h2>üìö Appendix: Technical Details</h2>

            <p>For those interested in the implementation details, the core components are:</p>

            <h3>AddressedStateAttention Module</h3>

            <pre><code>class AddressedStateAttention(nn.Module):
    """
    Tiered slot-based attention with:
    - Private slots (per-head)
    - Layer-shared keys (per-layer, across heads)
    - Global keys (across all layers)
    """

    def __init__(
        self,
        embed_dim: int,
        num_heads: int = 8,
        num_slots_private: int = 24,
        num_slots_total: int = 32,
        # ... many knobs for position encoding,
        # temperature, refinement, etc.
    ):
        # Slot keys (learned, static)
        self.slot_keys_private = nn.Parameter(...)
        # Layer-shared and global keys passed in from LM

        # Separate projections for write and read operations
        self.Wk_write = nn.Linear(embed_dim, embed_dim, bias=False)
        self.Wv_write = nn.Linear(embed_dim, embed_dim, bias=False)
        self.Wq_read  = nn.Linear(embed_dim, embed_dim, bias=False)
        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)

        # Slot-space refinement (optional)
        self.slotspace_refine = SlotspaceRefine(...)</code></pre>

            <h3>Core Operations</h3>

            <p><strong>Write (token ‚Üí slots):</strong></p>
            <pre><code>write_logits  = einsum('bhqd,hkd->bhqk', token_keys, slot_keys)
write_weights = softmax(write_logits / write_temperature)
slot_updates  = einsum('bhqk,bhqd->hkd', write_weights, values)</code></pre>

            <p><strong>Read (slots ‚Üí token):</strong></p>
            <pre><code>read_logits  = einsum('bhqd,hkd->bhqk', queries, slot_keys)
read_weights = softmax(read_logits / read_temperature)
output       = einsum('bhqk,hkd->bhqd', read_weights, slot_state)</code></pre>

            <p><strong>Refinement (optional):</strong></p>
            <pre><code>u     = slot_in(read_weights)              # [B, H, T, d_slot]
delta = slot_out(linear_attention(u))      # [B, H, T, d]
output = output + gate * delta</code></pre>

            <h3>Slot Sharing Structure</h3>

            <pre><code># In the language model:
self.slot_keys_global = nn.Parameter(...)     # [K_global, d]
self.slot_keys_layer  = nn.ParameterList([    # per layer
    nn.Parameter(...) for _ in range(num_layers)
])

# In each ASA forward:
slot_keys = concat(
    self.slot_keys_private,   # [H, K_private, d]
    layer_shared_keys,        # [K_layer, d]  -> broadcast to [H, K_layer, d]
    global_keys               # [K_global, d] -> broadcast to [H, K_global, d]
)  # Result: [H, K_total, d]</code></pre>

            <p>The full implementation includes:</p>
            <ul>
                <li>RoPE positional encoding (on keys)</li>
                <li>ALiBi write bias (learnable strength)</li>
                <li>Content-conditioned read terms</li>
                <li>Slot dropout (for regularization)</li>
                <li>Chunked computation (for memory efficiency)</li>
                <li>Info/analysis hooks (for interpretability)</li>
            </ul>

            <p style="margin-top: 30px;"><em>Code is available at [repository link].</em></p>

            <hr />

            <p style="text-align: center; font-style: italic; margin: 30px 0;">
                <strong>Thanks for reading!</strong> If you made it this far, you're probably the kind of person who enjoys digging into how things actually work. I hope the office building metaphor helped make the technical details more concrete. Questions, thoughts, or "wait, what about...?" reactions are all welcome.
            </p>
        </div>

        <div class="content-section">
            <h2>üí¨ Token Reviews: Yelp for Language Models</h2>
            <p><em>What do the tokens themselves think about their journey through the building?</em></p>

            <div class="satisfaction-meter">
                <h3>Token Satisfaction Survey üìä</h3>
                <div>
                    <strong>Function words:</strong> 2.3/5 stars ‚≠ê‚≠ê
                    <div class="meter-bar">
                        <div class="meter-fill" data-target="46" style="width: 0%;">46%</div>
                    </div>
                </div>
                <div>
                    <strong>Content words:</strong> 4.7/5 stars ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
                    <div class="meter-bar">
                        <div class="meter-fill" data-target="94" style="width: 0%;">94%</div>
                    </div>
                </div>
                <div>
                    <strong>Punctuation:</strong> 5.0/5 stars ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (but only 3 responses)
                    <div class="meter-bar">
                        <div class="meter-fill" data-target="100" style="width: 0%;">100%</div>
                    </div>
                </div>
            </div>

            <!-- Comments unchanged (your originals) -->
            <div class="comment">
                <div class="comment-header">
                    <span class="token-name">TheToken_42</span>
                    <span>‚≠ê‚≠ê - Posted 2 hours ago</span>
                </div>
                <p><strong>token: "the"</strong></p>
                <p>Okay so I've been through this building HUNDREDS of times now and I have some FEEDBACK.</p>
                <p>First of all‚Äîwhy am I always getting routed to the boring notebooks? Every single time I walk into Layer 0, the workers take one look at me and go "yep, global-shared slot #3, structural marker, next." Meanwhile "Stanford" over there gets written into THREE private notebooks with 30% write weight EACH. What am I, chopped liver?</p>
                <p>I'm filing a complaint with Building Management (the LayerNorm module). This is not what I signed up for when I got tokenized.</p>
            </div>

            <div class="comment">
                <div class="comment-header">
                    <span class="token-name">ProperNoun_Stanford</span>
                    <span>Posted 1 hour ago</span>
                </div>
                <p><strong>token: "Stanford"</strong></p>
                <p>Replying to @TheToken_42</p>
                <p>Okay okay, I hear you, and that does sound frustrating. But like... isn't that kind of your JOB though?</p>
                <p>I don't mean that in a mean way! I'm just saying‚Äîyou're a function word. You're DESIGNED to be structural glue. The fact that they're routing you to global-shared slots means you're actually REALLY IMPORTANT for every token that comes after you. Those slots persist! They're the foundation everyone else builds on!</p>
                <p>Also‚Äîand I'm just being real here‚Äîdo you WANT to be written into private notebooks on every layer? That sounds exhausting.</p>
            </div>

            <div class="comment">
                <div class="comment-header">
                    <span class="token-name">TheToken_42</span>
                    <span>Posted 1 hour ago</span>
                </div>
                <p><strong>token: "the"</strong></p>
                <p>Replying to @ProperNoun_Stanford</p>
                <p>"Isn't that kind of your JOB though?"</p>
                <p>WOW. Just WOW.</p>
                <p>So when YOU get personalized attention and 30% of a worker's write budget, that's "good service." But when I get routed to shared infrastructure, it's "doing my job" and I should be GRATEFUL?</p>
                <p>And don't give me that "you're the foundation" nonsense. You know what a foundation is? It's the thing nobody notices until it breaks.</p>
            </div>

            <div class="comment">
                <div class="comment-header">
                    <span class="token-name">SemiColon_87</span>
                    <span class="star-rating">‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê - Posted 15 minutes ago</span>
                </div>
                <p><strong>token: ";"</strong></p>
                <p>lmao you two are arguing about notebook allocation while I'm over here getting written into the SAME STRUCTURAL SLOT at Layer 4 for the 10,000th time in a row with 95% write weight and you don't hear ME complaining</p>
                <p>also @TheToken_42 they put both of us in the global-shared slots and i have to share that memory with 47 other punctuation marks so like... maybe count your blessings that at least you have semantic content????</p>
                <p>anyway building's great, workers are efficient, 5 stars, stop whining</p>
            </div>

            <div class="comment official-response">
                <div class="comment-header">
                    <span class="token-name">[LM_OFFICE_BUILDING]</span>
                    <span>Official Response - Posted 5 minutes ago</span>
                </div>
                <p>Thank you all for your feedback! We value every token's experience at our facility.</p>
                <p><strong>Please note:</strong></p>
                <ul>
                    <li>Notebook allocation is determined by learned optimization, not favoritism</li>
                    <li>All tokens receive appropriate processing based on their role in next-token prediction</li>
                    <li>Routing entropy metrics are a feature, not a bug</li>
                    <li>Structural importance ‚â† lower value</li>
                </ul>
                <p>We appreciate your continued participation in language modeling.</p>
                <p><em>This is an automated response. Please do not reply to this message.</em></p>
            </div>

            <div class="comment">
                <div class="comment-header">
                    <span class="token-name">TheToken_42</span>
                    <span>Posted 2 minutes ago</span>
                </div>
                <p><strong>token: "the"</strong></p>
                <p>Replying to @[LM_OFFICE_BUILDING]</p>
                <p>"Token satisfaction survey results: Function words: 2.3/5 stars"</p>
                <p><strong>SEE??? IT'S NOT JUST ME!!!</strong></p>
            </div>

            <div class="sticky-note" style="text-align: center; margin-top: 30px;">
                <strong>üìù Disclaimer:</strong><br />
                No tokens were harmed in the making of this comment section. All routing patterns and notebook allocations were performed by consenting, properly-trained attention heads. Function words received appropriate structural consideration. The building apologizes for nothing.
            </div>
        </div>
    </div>

    <!-- Interactive Token Generator Button -->
    <div class="token-generator">
        <button class="token-btn" id="spawnBtn" type="button">üé´ Spawn Random Token!</button>
    </div>

    <!-- Modal -->
    <!-- Inline tooltip -->
    <div class="info-tooltip" id="infoTooltip">
        <div class="info-tooltip-header" id="tooltipHeader">Info</div>
        <div class="info-tooltip-body" id="tooltipBody">
            <!-- populated by JS -->
        </div>
    </div>

    <script>
        // =========================
        // Small UX infrastructure
        // =========================
        const tooltip = document.getElementById('infoTooltip');
        const tooltipHeader = document.getElementById('tooltipHeader');
        const tooltipBody = document.getElementById('tooltipBody');
        let hideTimeout;

        const showTooltip = (title, html, targetElement) => {
            // Clear any existing hide timeout
            if (hideTimeout) clearTimeout(hideTimeout);

            // Update content
            tooltipHeader.textContent = title;
            tooltipBody.innerHTML = html;

            // Position near the clicked element
            const rect = targetElement.getBoundingClientRect();
            const tooltipRect = tooltip.getBoundingClientRect();
            
            // Try to position below the element
            let top = rect.bottom + 10;
            let left = rect.left;

            // If it would go off the bottom of the screen, position above instead
            if (top + 300 > window.innerHeight) {
                top = rect.top - 310;
            }

            // Keep it within horizontal bounds
            if (left + 400 > window.innerWidth) {
                left = window.innerWidth - 420;
            }
            if (left < 20) {
                left = 20;
            }

            // Apply position
            tooltip.style.top = `${top}px`;
            tooltip.style.left = `${left}px`;

            // Show tooltip
            tooltip.classList.add('show');

            // Auto-hide after 4 seconds
            hideTimeout = setTimeout(() => {
                hideTooltip();
            }, 4000);
        };

        const hideTooltip = () => {
            tooltip.classList.remove('show');
            if (hideTimeout) clearTimeout(hideTimeout);
        };

        // Click anywhere to dismiss tooltip
        document.addEventListener('click', (e) => {
            if (tooltip.classList.contains('show') && 
                !tooltip.contains(e.target) && 
                !e.target.closest('.floor') && 
                !e.target.closest('.notebook')) {
                hideTooltip();
            }
        });

        // Helper for tap/click interactions (prevents double-firing on mobile)
        const bindTap = (el, fn) => {
            el.addEventListener('pointerup', (e) => {
                // Do not preventDefault here: allow scrolling.
                fn(e);
            });
        };

        // =========================
        // Building visualization
        // =========================
        const buildingViz = document.getElementById('buildingViz');

        // NOTE: Labels aligned to your narrative: early=content, mid=structure, late=integration.
        const floors = [
            { num: 10, desc: "üéØ Final predictions ‚Äî choose next token" },
            { num: 9,  desc: "üîÆ Integration ‚Äî meaning in context" },
            { num: 8,  desc: "üß© Semantic composition" },
            { num: 7,  desc: "üìê Longer-range dependencies" },
            { num: 6,  desc: "üîÅ Integration begins (structure + content)" },
            { num: 5,  desc: "üå≥ Structure dominates (syntax/grammar)" },
            { num: 4,  desc: "‚ú® Function words & syntax cues" },
            { num: 3,  desc: "üß± Structural patterns consolidate" },
            { num: 2,  desc: "üî§ Early lexical processing" },
            { num: 1,  desc: "üèÅ Content first ‚Äî what is this token?" }
        ];

        floors.forEach(floor => {
            const floorDiv = document.createElement('div');
            floorDiv.className = 'floor';
            floorDiv.innerHTML = `
                <div class="floor-number">Floor ${floor.num}</div>
                <div class="floor-info">${floor.desc}</div>
            `;

            bindTap(floorDiv, (e) => {
                showTooltip(
                    `Floor ${floor.num}`,
                    `
                    <p><strong>${floor.desc}</strong></p>
                    <p>This floor has <strong>4 workers</strong> (attention heads) and <strong>8 notebooks</strong> per worker:</p>
                    <ul>
                        <li>4 private (per head)</li>
                        <li>2 layer-shared tab labels (per layer)</li>
                        <li>2 global tab labels (across layers)</li>
                    </ul>
                    <p><em>Remember:</em> tab labels (keys) can be shared; the <strong>state</strong> is updated causally as the prefix is scanned.</p>
                    `,
                    floorDiv
                );
            });

            buildingViz.appendChild(floorDiv);
        });

        // =========================
        // Token spawning
        // =========================
        const tokens = [
            '"the"', '"a"', '"Stanford"', '"university"', '"is"', '";"',
            '"!"', '"machine"', '"learning"', '"model"', '"attention"',
            '"slot"', '"notebook"', '"worker"', '"layer"', '"building"'
        ];

        function spawnToken() {
            const token = tokens[Math.floor(Math.random() * tokens.length)];
            const floatingToken = document.createElement('div');
            floatingToken.className = 'floating-token';
            floatingToken.textContent = token;

            const maxWidth = Math.max(0, window.innerWidth - 100);
            floatingToken.style.left = (Math.random() * maxWidth) + 'px';
            floatingToken.style.bottom = '120px';

            document.body.appendChild(floatingToken);

            setTimeout(() => floatingToken.remove(), 4000);
        }

        document.getElementById('spawnBtn').addEventListener('click', spawnToken);

        // Less frequent random spawns (and pause when tab hidden)
        const isMobile = window.innerWidth < 768;
        const spawnInterval = isMobile ? 8000 : 5000;

        let spawnTimer = setInterval(() => {
            if (!document.hidden && Math.random() < 0.25) spawnToken();
        }, spawnInterval);

        document.addEventListener('visibilitychange', () => {
            if (document.hidden) return;
            // no-op; interval guard already checks document.hidden
        });

        // =========================
        // Satisfaction meter animation
        // =========================
        window.addEventListener('load', () => {
            document.querySelectorAll('.meter-fill').forEach((fill, i) => {
                const target = fill.getAttribute('data-target') || "0";
                setTimeout(() => {
                    fill.style.width = `${target}%`;
                }, i * 200);
            });
        });

        // =========================
        // Notebook easter eggs (modal, not alerts)
        // =========================
        const notebookMessages = {
            private: [
                "Private notebook: head-local scratchpad. Great for idiosyncratic cues and local heuristics.",
                "Private notebook: where a head can store niche signals without enforcing global agreement."
            ],
            layer: [
                "Layer-shared tab labels: a shared role vocabulary across heads on this floor.",
                "Layer-shared addressing helps prevent drift: multiple heads can agree on what a role means."
            ],
            global: [
                "Global tab labels: a tiny shared role vocabulary across the building (across layers).",
                "Global addressing supports stable, reusable roles that remain comparable throughout the stack."
            ]
        };

        document.querySelectorAll('.notebook').forEach(notebook => {
            bindTap(notebook, (e) => {
                const kind = notebook.getAttribute('data-kind') || 'private';
                const msgs = notebookMessages[kind] || notebookMessages.private;
                const msg = msgs[Math.floor(Math.random() * msgs.length)];

                showTooltip(
                    "Notebook",
                    `<p>${msg}</p><p style="margin-top:10px;"><code>Slots = role accumulators.</code> Keys define addresses; state carries the prefix summary.</p>`,
                    notebook
                );
            });
        });
    </script>
</body>
</html>                <li>Slots mix structural and semantic roles again</li>
            </ul>

            <p>These layers answer: <strong>"What does this token mean here, now?"</strong></p>
        </div>

        <div class="content-section">
            <h2>üìù Step 5: Updating the Sticky Note (Routing)</h2>

            <p>After all workers finish writing and reading, something interesting happens.</p>

            <p>A <strong>coordinator</strong> computes‚Äîon this floor‚Äîhow strongly this token should consult each role notebook. The coordinator looks at:</p>
            <ul>
                <li><strong>Not</strong> the notes themselves</li>
                <li>Just <strong>which roles mattered</strong>, and <strong>how much</strong></li>
            </ul>

            <p>This reading pattern contributes to the token‚Äôs updated sticky note (its new residual-stream representation):</p>
            <ul>
                <li>Emphasizing certain roles</li>
                <li>De-emphasizing others</li>
                <li>Shaping the representation that moves to the next floor</li>
            </ul>

            <div class="info-box">
                <strong>This is what routing really is in this model:</strong><br />
                Not selecting content ‚Äî selecting <strong>roles</strong>
            </div>

            <p>The routing weights don‚Äôt tell us <em>what</em> the token saw, but <em>how</em> it used available information channels. Importantly, routing is recomputed at each floor based on the token‚Äôs current representation‚Äîit's not a static instruction carried upward.</p>
        </div>

        <div class="content-section">
            <h2>üõó Step 6: Refinement Happens in the Elevator</h2>

            <p>As the token rides the elevator between floors, there‚Äôs a quiet observer watching everything.</p>

            <p>This observer:</p>
            <ul>
                <li>Watches routing patterns over time</li>
                <li>Never sees the token itself</li>
                <li>Only sees <strong>how roles are being used</strong></li>
            </ul>

            <p>This is the key insight: refinement is <strong>meta</strong> because it adjusts based on how you routed (which notebooks you consulted), not raw token content. Crucially, <strong>refinement sees read_weights, not slot contents</strong>.</p>

            <p>Occasionally, the observer nudges the sticky note:</p>
            <ul>
                <li>Suppress redundancy</li>
                <li>Amplify underused structure</li>
                <li>Stabilize drifting behavior</li>
            </ul>

            <p>This corresponds to <strong>slot-space refinement</strong>:</p>

            <pre><code># Refinement operates on read weights (not content)
u     = slot_in(read_weights)           # compress read pattern
delta = slot_out(linear_attention(u))   # refine in lower-dim space
output += gate * delta                  # add back gated correction</code></pre>

            <p><strong>Interpretation:</strong></p>
            <ul>
                <li>The model reflects on <strong>how it used roles</strong></li>
                <li>Not on <strong>what the roles contained</strong></li>
                <li>Adjustments are typically stabilizing (and should remain bounded)</li>
            </ul>

            <div class="sticky-note">
                <strong>Importantly:</strong><br />
                ‚Ä¢ Turning refinement off <strong>can</strong> degrade generation<br />
                ‚Ä¢ Refinement should not <strong>overwhelm</strong> base behavior<br />
                ‚Ä¢ It‚Äôs corrective, not controlling
            </div>
        </div>

        <div class="content-section">
            <h2>üèÜ The Top Floor: Choosing the Next Token</h2>

            <p>At decision time, the ‚Äúboss‚Äù (the output head) doesn‚Äôt see raw past tokens directly. The only things available are:</p>
            <ol>
                <li>The current token‚Äôs representation after the stack (the accumulated sticky note)</li>
                <li>Whatever structure the model has distilled into internal role channels while scanning the prefix</li>
            </ol>

            <p>The output head maps that final representation to a distribution over the vocabulary.</p>

            <p>When the token finally reaches the top floor:</p>
            <ul>
                <li>The sticky note encodes <strong>layered context</strong> from all floors</li>
                <li>Slot state holds <strong>prefix summaries</strong> organized by roles</li>
                <li>Private capacity can capture <strong>local nuance</strong> without breaking shared conventions</li>
            </ul>

            <p>The model then answers a single question:</p>

            <blockquote style="background: var(--office-tan); border-left: 4px solid var(--coffee-brown); padding: 20px; margin: 20px 0; font-style: italic;">
                If the line were one token longer, which of the 50,000+ tokens would fit best here?
            </blockquote>

            <pre><code>x        = layer_norm(x)      # final normalization
logits   = lm_head(x)         # [batch, time, vocab_size]
next_tok = sample(logits[:, -1, :])</code></pre>

            <p>Then the next token enters the building, and the whole process repeats.</p>
        </div>

        <div class="content-section">
            <h2>üîë Why Shared Slots Matter (A Lot)</h2>

            <p>Let‚Äôs return to those notebooks. Remember how some notebooks are private, some share tab labels within a floor, and some share tab labels across the entire building?</p>

            <p>Analysis often shows a layerwise shift:</p>

            <table style="width: 100%; border-collapse: collapse; margin: 20px 0;">
                <thead>
                    <tr style="background: var(--coffee-brown); color: white;">
                        <th style="padding: 12px; border: 2px solid var(--ink-black); text-align: left;">Layer Range</th>
                        <th style="padding: 12px; border: 2px solid var(--ink-black); text-align: left;">Typical Emphasis</th>
                    </tr>
                </thead>
                <tbody>
                    <tr style="background: white;">
                        <td style="padding: 12px; border: 2px solid var(--ink-black);">Lower layers</td>
                        <td style="padding: 12px; border: 2px solid var(--ink-black);">Content-heavy (entities, topics)</td>
                    </tr>
                    <tr style="background: var(--office-tan);">
                        <td style="padding: 12px; border: 2px solid var(--ink-black);">Mid layers</td>
                        <td style="padding: 12px; border: 2px solid var(--ink-black);">Structure-heavy (syntax, function words)</td>
                    </tr>
                    <tr style="background: white;">
                        <td style="padding: 12px; border: 2px solid var(--ink-black);">Upper layers</td>
                        <td style="padding: 12px; border: 2px solid var(--ink-black);">Mixed integration (meaning-in-context)</td>
                    </tr>
                </tbody>
            </table>

            <p>This emerges because of <strong>shared addressing</strong>.</p>

            <h3>Layer-Shared Keysets</h3>
            <ul>
                <li>Encourage consistency across heads</li>
                <li>Stabilize grammatical roles</li>
                <li>Reduce ‚Äúhead drift‚Äù by giving everyone a common role vocabulary</li>
            </ul>

            <h3>Global Keysets</h3>
            <ul>
                <li>Provide a small, persistent role vocabulary across the full stack</li>
                <li>Can support long-range discourse regularities (topic continuity, mode shifts, etc.)</li>
            </ul>

            <p>In code, this is visible in how slot keys are constructed:</p>

            <pre><code>slot_keys = concat(
    private_keys,      # [H, K_private, d]
    layer_shared_keys, # [K_layer, d]  (broadcast across heads)
    global_shared_keys # [K_global, d] (broadcast across heads and layers)
)</code></pre>

            <p>This single concatenation explains a lot of behavior. Workers have access to private scratchpads and shared role conventions, and they learn when to use which.</p>
        </div>

        <div class="content-section">
            <h2>üåü Why Structure Emerges Without Supervision</h2>

            <p>No slot is labeled ‚Äúnoun‚Äù or ‚Äúverb‚Äù or ‚Äúsubject.‚Äù</p>

            <p>Structure emerges because:</p>
            <ul>
                <li><strong>Structure is reusable</strong> ‚Äî the same patterns apply everywhere</li>
                <li><strong>Structure compresses history efficiently</strong> ‚Äî roles summarize more than raw token lists</li>
                <li><strong>Structure improves predictability</strong> ‚Äî knowing ‚Äúwe‚Äôre in a prepositional phrase‚Äù constrains what comes next</li>
            </ul>

            <p>Slots that encode structure:</p>
            <ul>
                <li>Get <strong>reused</strong> frequently</li>
                <li><strong>Accumulate mass</strong> (higher write/read weight)</li>
                <li>Become <strong>stable attractors</strong> in slot space</li>
            </ul>

            <p>This is selection pressure from the objective, not hard coding.</p>
        </div>

        <div class="content-section">
            <h2>ü§î Why This Is a Different Kind of Model</h2>

            <p>At no point does this system:</p>
            <ul>
                <li>Store a single ‚Äúmeaning‚Äù for a token</li>
                <li>Commit to one role</li>
                <li>Require a quadratic attention map between tokens</li>
            </ul>

            <p>Instead:</p>
            <ul>
                <li><strong>Meaning is distributed</strong> across multiple slots</li>
                <li><strong>Roles are soft and reusable</strong> across time and context</li>
                <li><strong>Structure and content trade dominance naturally by layer</strong></li>
            </ul>

            <p>This helps explain why you can see:</p>
            <ul>
                <li>Surprisingly strong performance at modest scale (efficient capacity use)</li>
                <li>Healthy entropy (no dead slots, no brittle gating)</li>
                <li>Emergent structure without explicit labels</li>
            </ul>
        </div>

        <!-- (Your remaining sections unchanged for brevity of this ‚Äúfinal pass‚Äù update)
             Keeping your Transformer vs not-Transformer, MoE discussion, and closing reflection as-is,
             with minor textual tightening already incorporated above. -->

        <div class="content-section">
            <h2>üìö Appendix: Technical Details</h2>

            <p>For those interested in the implementation details, the core components are:</p>

            <h3>AddressedStateAttention Module</h3>

            <pre><code>class AddressedStateAttention(nn.Module):
    """
    Tiered slot-based attention with:
    - Private slots (per-head)
    - Layer-shared keys (per-layer, across heads)
    - Global keys (across all layers)
    """

    def __init__(
        self,
        embed_dim: int,
        num_heads: int = 8,
        num_slots_private: int = 24,
        num_slots_total: int = 32,
        # ... many knobs for position encoding,
        # temperature, refinement, etc.
    ):
        # Slot keys (learned, static)
        self.slot_keys_private = nn.Parameter(...)
        # Layer-shared and global keys passed in from LM

        # Separate projections for write and read operations
        self.Wk_write = nn.Linear(embed_dim, embed_dim, bias=False)
        self.Wv_write = nn.Linear(embed_dim, embed_dim, bias=False)
        self.Wq_read  = nn.Linear(embed_dim, embed_dim, bias=False)
        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)

        # Slot-space refinement (optional)
        self.slotspace_refine = SlotspaceRefine(...)</code></pre>

            <h3>Core Operations</h3>

            <p><strong>Write (token ‚Üí slots):</strong></p>
            <pre><code>write_logits  = einsum('bhqd,hkd->bhqk', token_keys, slot_keys)
write_weights = softmax(write_logits / write_temperature)
slot_updates  = einsum('bhqk,bhqd->hkd', write_weights, values)</code></pre>

            <p><strong>Read (slots ‚Üí token):</strong></p>
            <pre><code>read_logits  = einsum('bhqd,hkd->bhqk', queries, slot_keys)
read_weights = softmax(read_logits / read_temperature)
output       = einsum('bhqk,hkd->bhqd', read_weights, slot_state)</code></pre>

            <p><strong>Refinement (optional):</strong></p>
            <pre><code>u     = slot_in(read_weights)              # [B, H, T, d_slot]
delta = slot_out(linear_attention(u))      # [B, H, T, d]
output = output + gate * delta</code></pre>

            <h3>Slot Sharing Structure</h3>

            <pre><code># In the language model:
self.slot_keys_global = nn.Parameter(...)     # [K_global, d]
self.slot_keys_layer  = nn.ParameterList([    # per layer
    nn.Parameter(...) for _ in range(num_layers)
])

# In each ASA forward:
slot_keys = concat(
    self.slot_keys_private,   # [H, K_private, d]
    layer_shared_keys,        # [K_layer, d]  -> broadcast to [H, K_layer, d]
    global_keys               # [K_global, d] -> broadcast to [H, K_global, d]
)  # Result: [H, K_total, d]</code></pre>

            <p>The full implementation includes:</p>
            <ul>
                <li>RoPE positional encoding (on keys)</li>
                <li>ALiBi write bias (learnable strength)</li>
                <li>Content-conditioned read terms</li>
                <li>Slot dropout (for regularization)</li>
                <li>Chunked computation (for memory efficiency)</li>
                <li>Info/analysis hooks (for interpretability)</li>
            </ul>

            <p style="margin-top: 30px;"><em>Code is available at [repository link].</em></p>

            <hr />

            <p style="text-align: center; font-style: italic; margin: 30px 0;">
                <strong>Thanks for reading!</strong> If you made it this far, you're probably the kind of person who enjoys digging into how things actually work. I hope the office building metaphor helped make the technical details more concrete. Questions, thoughts, or "wait, what about...?" reactions are all welcome.
            </p>
        </div>

        <div class="content-section">
            <h2>üí¨ Token Reviews: Yelp for Language Models</h2>
            <p><em>What do the tokens themselves think about their journey through the building?</em></p>

            <div class="satisfaction-meter">
                <h3>Token Satisfaction Survey üìä</h3>
                <div>
                    <strong>Function words:</strong> 2.3/5 stars ‚≠ê‚≠ê
                    <div class="meter-bar">
                        <div class="meter-fill" data-target="46" style="width: 0%;">46%</div>
                    </div>
                </div>
                <div>
                    <strong>Content words:</strong> 4.7/5 stars ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
                    <div class="meter-bar">
                        <div class="meter-fill" data-target="94" style="width: 0%;">94%</div>
                    </div>
                </div>
                <div>
                    <strong>Punctuation:</strong> 5.0/5 stars ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (but only 3 responses)
                    <div class="meter-bar">
                        <div class="meter-fill" data-target="100" style="width: 0%;">100%</div>
                    </div>
                </div>
            </div>

            <!-- Comments unchanged (your originals) -->
            <div class="comment">
                <div class="comment-header">
                    <span class="token-name">TheToken_42</span>
                    <span>‚≠ê‚≠ê - Posted 2 hours ago</span>
                </div>
                <p><strong>token: "the"</strong></p>
                <p>Okay so I've been through this building HUNDREDS of times now and I have some FEEDBACK.</p>
                <p>First of all‚Äîwhy am I always getting routed to the boring notebooks? Every single time I walk into Layer 0, the workers take one look at me and go "yep, global-shared slot #3, structural marker, next." Meanwhile "Stanford" over there gets written into THREE private notebooks with 30% write weight EACH. What am I, chopped liver?</p>
                <p>I'm filing a complaint with Building Management (the LayerNorm module). This is not what I signed up for when I got tokenized.</p>
            </div>

            <div class="comment">
                <div class="comment-header">
                    <span class="token-name">ProperNoun_Stanford</span>
                    <span>Posted 1 hour ago</span>
                </div>
                <p><strong>token: "Stanford"</strong></p>
                <p>Replying to @TheToken_42</p>
                <p>Okay okay, I hear you, and that does sound frustrating. But like... isn't that kind of your JOB though?</p>
                <p>I don't mean that in a mean way! I'm just saying‚Äîyou're a function word. You're DESIGNED to be structural glue. The fact that they're routing you to global-shared slots means you're actually REALLY IMPORTANT for every token that comes after you. Those slots persist! They're the foundation everyone else builds on!</p>
                <p>Also‚Äîand I'm just being real here‚Äîdo you WANT to be written into private notebooks on every layer? That sounds exhausting.</p>
            </div>

            <div class="comment">
                <div class="comment-header">
                    <span class="token-name">TheToken_42</span>
                    <span>Posted 1 hour ago</span>
                </div>
                <p><strong>token: "the"</strong></p>
                <p>Replying to @ProperNoun_Stanford</p>
                <p>"Isn't that kind of your JOB though?"</p>
                <p>WOW. Just WOW.</p>
                <p>So when YOU get personalized attention and 30% of a worker's write budget, that's "good service." But when I get routed to shared infrastructure, it's "doing my job" and I should be GRATEFUL?</p>
                <p>And don't give me that "you're the foundation" nonsense. You know what a foundation is? It's the thing nobody notices until it breaks.</p>
            </div>

            <div class="comment">
                <div class="comment-header">
                    <span class="token-name">SemiColon_87</span>
                    <span class="star-rating">‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê - Posted 15 minutes ago</span>
                </div>
                <p><strong>token: ";"</strong></p>
                <p>lmao you two are arguing about notebook allocation while I'm over here getting written into the SAME STRUCTURAL SLOT at Layer 4 for the 10,000th time in a row with 95% write weight and you don't hear ME complaining</p>
                <p>also @TheToken_42 they put both of us in the global-shared slots and i have to share that memory with 47 other punctuation marks so like... maybe count your blessings that at least you have semantic content????</p>
                <p>anyway building's great, workers are efficient, 5 stars, stop whining</p>
            </div>

            <div class="comment official-response">
                <div class="comment-header">
                    <span class="token-name">[LM_OFFICE_BUILDING]</span>
                    <span>Official Response - Posted 5 minutes ago</span>
                </div>
                <p>Thank you all for your feedback! We value every token's experience at our facility.</p>
                <p><strong>Please note:</strong></p>
                <ul>
                    <li>Notebook allocation is determined by learned optimization, not favoritism</li>
                    <li>All tokens receive appropriate processing based on their role in next-token prediction</li>
                    <li>Routing entropy metrics are a feature, not a bug</li>
                    <li>Structural importance ‚â† lower value</li>
                </ul>
                <p>We appreciate your continued participation in language modeling.</p>
                <p><em>This is an automated response. Please do not reply to this message.</em></p>
            </div>

            <div class="comment">
                <div class="comment-header">
                    <span class="token-name">TheToken_42</span>
                    <span>Posted 2 minutes ago</span>
                </div>
                <p><strong>token: "the"</strong></p>
                <p>Replying to @[LM_OFFICE_BUILDING]</p>
                <p>"Token satisfaction survey results: Function words: 2.3/5 stars"</p>
                <p><strong>SEE??? IT'S NOT JUST ME!!!</strong></p>
            </div>

            <div class="sticky-note" style="text-align: center; margin-top: 30px;">
                <strong>üìù Disclaimer:</strong><br />
                No tokens were harmed in the making of this comment section. All routing patterns and notebook allocations were performed by consenting, properly-trained attention heads. Function words received appropriate structural consideration. The building apologizes for nothing.
            </div>
        </div>
    </div>

    <!-- Interactive Token Generator Button -->
    <div class="token-generator">
        <button class="token-btn" id="spawnBtn" type="button">üé´ Spawn Random Token!</button>
    </div>

    <!-- Modal -->
    <div class="modal-backdrop" id="modalBackdrop" role="dialog" aria-modal="true" aria-labelledby="modalTitle">
        <div class="modal">
            <div class="modal-header">
                <div class="modal-title" id="modalTitle">Info</div>
                <button class="modal-close" id="modalClose" type="button" aria-label="Close">Close</button>
            </div>
            <div class="modal-body" id="modalBody">
                <!-- populated by JS -->
            </div>
        </div>
    </div>

    <script>
        // =========================
        // Small UX infrastructure
        // =========================
        const modalBackdrop = document.getElementById('modalBackdrop');
        const modalTitle = document.getElementById('modalTitle');
        const modalBody = document.getElementById('modalBody');
        const modalClose = document.getElementById('modalClose');

        const openModal = (title, html) => {
            modalTitle.textContent = title;
            modalBody.innerHTML = html;
            modalBackdrop.style.display = 'flex';
            modalClose.focus();
        };

        const closeModal = () => {
            modalBackdrop.style.display = 'none';
        };

        modalClose.addEventListener('click', closeModal);
        modalBackdrop.addEventListener('click', (e) => {
            if (e.target === modalBackdrop) closeModal();
        });

        window.addEventListener('keydown', (e) => {
            if (e.key === 'Escape' && modalBackdrop.style.display === 'flex') closeModal();
        });

        // Helper for tap/click interactions (prevents double-firing on mobile)
        const bindTap = (el, fn) => {
            el.addEventListener('pointerup', (e) => {
                // Do not preventDefault here: allow scrolling.
                fn(e);
            });
        };

        // =========================
        // Building visualization
        // =========================
        const buildingViz = document.getElementById('buildingViz');

        // NOTE: Labels aligned to your narrative: early=content, mid=structure, late=integration.
        const floors = [
            { num: 10, desc: "üéØ Final predictions ‚Äî choose next token" },
            { num: 9,  desc: "üîÆ Integration ‚Äî meaning in context" },
            { num: 8,  desc: "üß© Semantic composition" },
            { num: 7,  desc: "üìê Longer-range dependencies" },
            { num: 6,  desc: "üîÅ Integration begins (structure + content)" },
            { num: 5,  desc: "üå≥ Structure dominates (syntax/grammar)" },
            { num: 4,  desc: "‚ú® Function words & syntax cues" },
            { num: 3,  desc: "üß± Structural patterns consolidate" },
            { num: 2,  desc: "üî§ Early lexical processing" },
            { num: 1,  desc: "üèÅ Content first ‚Äî what is this token?" }
        ];

        floors.forEach(floor => {
            const floorDiv = document.createElement('div');
            floorDiv.className = 'floor';
            floorDiv.innerHTML = `
                <div class="floor-number">Floor ${floor.num}</div>
                <div class="floor-info">${floor.desc}</div>
            `;

            bindTap(floorDiv, () => {
                openModal(
                    `Floor ${floor.num}`,
                    `
                    <p><strong>${floor.desc}</strong></p>
                    <p>This floor has <strong>4 workers</strong> (attention heads) and <strong>8 notebooks</strong> per worker:</p>
                    <ul>
                        <li>4 private (per head)</li>
                        <li>2 layer-shared tab labels (per layer)</li>
                        <li>2 global tab labels (across layers)</li>
                    </ul>
                    <p><em>Remember:</em> tab labels (keys) can be shared; the <strong>state</strong> is updated causally as the prefix is scanned.</p>
                    `
                );
            });

            buildingViz.appendChild(floorDiv);
        });

        // =========================
        // Token spawning
        // =========================
        const tokens = [
            '"the"', '"a"', '"Stanford"', '"university"', '"is"', '";"',
            '"!"', '"machine"', '"learning"', '"model"', '"attention"',
            '"slot"', '"notebook"', '"worker"', '"layer"', '"building"'
        ];

        function spawnToken() {
            const token = tokens[Math.floor(Math.random() * tokens.length)];
            const floatingToken = document.createElement('div');
            floatingToken.className = 'floating-token';
            floatingToken.textContent = token;

            const maxWidth = Math.max(0, window.innerWidth - 100);
            floatingToken.style.left = (Math.random() * maxWidth) + 'px';
            floatingToken.style.bottom = '120px';

            document.body.appendChild(floatingToken);

            setTimeout(() => floatingToken.remove(), 4000);
        }

        document.getElementById('spawnBtn').addEventListener('click', spawnToken);

        // Less frequent random spawns (and pause when tab hidden)
        const isMobile = window.innerWidth < 768;
        const spawnInterval = isMobile ? 8000 : 5000;

        let spawnTimer = setInterval(() => {
            if (!document.hidden && Math.random() < 0.25) spawnToken();
        }, spawnInterval);

        document.addEventListener('visibilitychange', () => {
            if (document.hidden) return;
            // no-op; interval guard already checks document.hidden
        });

        // =========================
        // Satisfaction meter animation
        // =========================
        window.addEventListener('load', () => {
            document.querySelectorAll('.meter-fill').forEach((fill, i) => {
                const target = fill.getAttribute('data-target') || "0";
                setTimeout(() => {
                    fill.style.width = `${target}%`;
                }, i * 200);
            });
        });

        // =========================
        // Notebook easter eggs (modal, not alerts)
        // =========================
        const notebookMessages = {
            private: [
                "Private notebook: head-local scratchpad. Great for idiosyncratic cues and local heuristics.",
                "Private notebook: where a head can store niche signals without enforcing global agreement."
            ],
            layer: [
                "Layer-shared tab labels: a shared role vocabulary across heads on this floor.",
                "Layer-shared addressing helps prevent drift: multiple heads can agree on what a role means."
            ],
            global: [
                "Global tab labels: a tiny shared role vocabulary across the building (across layers).",
                "Global addressing supports stable, reusable roles that remain comparable throughout the stack."
            ]
        };

        document.querySelectorAll('.notebook').forEach(notebook => {
            bindTap(notebook, () => {
                const kind = notebook.getAttribute('data-kind') || 'private';
                const msgs = notebookMessages[kind] || notebookMessages.private;
                const msg = msgs[Math.floor(Math.random() * msgs.length)];

                openModal(
                    "Notebook",
                    `<p>${msg}</p><p style="margin-top:10px;"><code>Slots = role accumulators.</code> Keys define addresses; state carries the prefix summary.</p>`
                );
            });
        });
    </script>
</body>
</html>
